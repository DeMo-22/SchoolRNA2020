---
title: '<img border="0" src="https://d1nhio0ox7pgb.cloudfront.net/_img/o_collection_png/green_dark_grey/512x512/plain/dictionary.png" width="40" height="40"> Glossary of terms'
output:
  html_document:
    keep_md: true
    toc: false
    toc_depth: 3
    code_folding: show
  pdf_document: default
editor_options: 
  chunk_output_type: console
---
***

<br/>

# Downloading data
***

<details>
<summary>Click to expand!</summary>

It is possible to download data from within Rstudio by using the bash code chunck. It is usually a standard to have a folder with all your raw data stored in a separate place from your code or analysis results.

How to run it:

```{bash, eval=F}
# use this instead of the default ```{r}
# ```{bash}

# make a data directory
mkdir data

# use curl to download the data
curl -o data/FILENAME.h5 -O http://FILE_PATH.h5
```

</details>

<br/>

<br/>

# Seurat Objects
***

<details>
<summary>**Reading files**</summary>
<p>

There are many formats available in which one can store single cell information, many of which cannot all be listed here. The most common formats are:

1. using tab-delimited matricies saved as `.csv`, `.tsv` or `.txt` and with and additional matrix containing the sample metadata, which is common for SMARTseq2 and related methods;
2. using a compressed **sparce matrix** file `.mtx` with annotations for genes and cells saved as `.tsv`, which was one of the defaults for 10X Chromium data; 
3. using HDF5 compressed file for in-file read-write access, which is now becoming the default method for storing single cell dataset (is the current default for 10X Chromium data). HDF5 in particular is fast, scalable and can load parts of the data that will be used at a time, and also can store the metadata in the same file, making it portable. It stores the data as binary compressed **sparce matrix** format.

How to run it:
```{r, eval=F,  results='hold'}
#From .csv .tsv .txt format (and then convert to sparse matrix)
raw_matrix <- read.delim(
  file = "data/folder_sample1.csv",
  row.names = 1 )

sparse_matrix <- Matrix::Matrix(
  data = raw_matrix, 
  sparse = T)
rm(raw_matrix)


#From .mtx format (it loads the files in the folder)
sparse_matrix <- Seurat::Read10X(
  data.dir = "data/folder_sample1")


#From .h5 format
sparse_matrix <- Seurat::Read10X_h5(
  filename = "data/matrix_file.h5",
  use.names = T)
```

Additionaly, one should also import any associated metadata file as a `data.frame`, which oposed to a `matrix` can store both numbers, characters, booleans and factors. 

```{r, eval=F,  results='hold'}
#From .csv .tsv format (and then convert to sparse matrix)
metadata <- read.delim(
  file = "data/metadata.csv",
  row.names = 1 )
```

</p>
</details>


<details>
<summary>**Creating Seurat objects**</summary>
<p>

In order to make the data analysis process a bit simpler, several single cell developers have implemented their own way of storing the data in a consize format in R and python (called **objects**).

We can now load the expression matricies into objects and then later merge them into a single merged object. Each analysis workflow (Seurat, Scater, Scranpy, etc) has its own way of storing data and here is how you can create a Seurat Object:

```{r, eval=F,  results='hold'}
SeuratObject <- CreateSeuratObject(  
  counts = sparse_matrix,
  assay = "RNA",
  project = "SAMPLE1",
  meta.data = metadata)
```


</p>
</details>

<details>
<summary>**Understanding Seurat objects**</summary>
<p>

One can check the dimentions and subset Seurat Objects as a data.frame:

```{r, eval=F, include=F, result='hide'}
#Checking the Seurat dimentions (Features x Cells)
dim(SeuratObject)

#Subsetting Features
SeuratObject <- SeuratObject[ vector_FEATURES_TO_USE , ]

#Subsetting Cells
SeuratObject <- SeuratObject[ , vector_CELLS_TO_USE ]
```

Seurat objects have a easy way to access their contents using the `@` or the `$` characters after the object name:
* The `@` attribute allows you to access to all analysis slots including: `assays`, `meta.data`, `graphs` and `reduction` slots.
* The `$` sign allows you to access the columns of the metadata (just like you normally would do in a data.frame) in your Seurat object directly so that `SeuratObject$column1` is equal to `SeuratObject@meta.data$column1`.

By default, the data is loaded into an `assay` slot named `RNA`, but you can change the names of the slots when creating them (e.g., when creating the seurat object, or computing the reductions). Therefore, check the options in each of the Seurat functions to know where you are storing the data. Each `assay` contains information about the raw counts (`counts`), the normalized counts (`data`), the scaled/regressed data (`scale.data`) as well as information about the dispersion of genes (`var`). Additional assays will be created when doing data analysis, for example when performing data integration, you might store the data as new `assay` or as a new `reduction` slot (depending on the integration method used).

```{r, eval=F, include=F, result='hide'}
SeuratObject@   # Type this an press TAB

SeuratObject@assays$   # Type this an press TAB

SeuratObject@assays$RNA@   # Type this an press TAB
```

</p>
</details>

<details>
<summary>**Add in a metadata column**</summary>
<p>

```{r, eval=F,  results='hold'}
SeuratObject$NEW_COLUMN_NAME <- SetNames( colnames(SeuratObject) , 
                                          c("VECTOR_CONTAINING_DATA_FOR_EACH_CELL") )
```

</p>
</details>

<details>
<summary>**Plotting functions**</summary>
<p>


The most common functions to use for plotting are the violin plot and the scatter plots for the dimensionality reduction calculated (you need to calculate it before using the function!).

To plot **continous** variables as **violin** (press TAB inside the functions for more options):

```{r, eval=F,  results='hold'}

VlnPlot(object = SeuratObject,
        group.by= "orig.ident",
        features = c("percent_mito"),
        pt.size = 0.1,
        ncol = 4,
        y.max = 100) + NoLegend()
```

To plot **CONTINOUS** variables as **scatter plot** using the `umap` reduction slot (press TAB inside the functions for more options):

```{r, eval=F,  results='hold'}
FeaturePlot(object = SeuratObject,
            features = c("FEATURE_1","FEATURE_2","FEATURE_3"),
            reduction = "umap",
            dims = c(1,2),
            order = T,
            pt.size = .1,
            ncol = 3)
```

To plot **CATEGORICAL** variables as **scatter plot** using the `umap` reduction slot (press TAB inside the functions for more options):

```{r, eval=F,  results='hold'}
DimPlot(object = SeuratObject,
        group.by = c("DATASET"),
        reduction = "umap",
        dims = c(1,2),
        pt.size = .1,
        label = T,
        ncol = 3)
```

Many other plotting functions are available, check `Seurat::` (then press tab and look for the functions with "Plot" in the name).

</p>
</details>

<details>
<summary>**Combine datasets**</summary>
<p>

```{r, eval=F,  results='hold'}
CombinedSeuratObject <- merge(
  x = SeuratObject1,
  y = c( SeuratObject2 ,
         SeuratObject3,
         SeuratObject4),
  add.cell.ids=c("Dataset1",
                 "Dataset2",
                 "Dataset3",
                 "Dataset4"))
```

</p>
</details>

<br/>

# Quality control
***

<details>
<summary>**Total number of features**</summary>
<p>

A standard approach is to filter cells with low amount of reads as well as genes that are present in at least a certain amount of cells. Here we will only consider cells with at least 200 detected genes and genes need to be expressed in at least 3 cells. Please note that those values are highly dependent on the library preparation method used. Extremely high number of detected genes could indicate doublets. However, depending on the cell type composition in your sample, you may have cells with higher number of genes (and also higher counts) from one cell type.

```{r, eval=F,  results='hold'}
VlnPlot(SeuratObject,
        group.by= "orig.ident",
        features = c("nFeature_RNA","nCount_RNA"),
        pt.size = 0.1,
        ncol = 4) + NoLegend()
```

</p>
</details>

<details>
<summary>**Gene QC**</summary>
<p>

In single cell, the most detected genes usually belong to housekeeping gene families, such as mitochondrial (MT-), ribossomal (RPL and RPS) and other structural proteins (i.e., ACTB, TMSB4X, B2M, EEF1A1).

```{r, eval=F,  }
#Compute the relative expression of each gene per cell
rel_expression <- Matrix::t( Matrix::t(SeuratObject@assays$RNA@counts) / Matrix::colSums(SeuratObject@assays$RNA@counts)) * 100
most_expressed <- sort(Matrix::rowSums( rel_expression ),T) / ncol(SeuratObject)

#Plot the relative expression of each gene per cell
par(mfrow=c(1,3),mar=c(4,6,1,1))
boxplot( as.matrix(Matrix::t(rel_expression[names(most_expressed[30:1]),])),cex=.1, las=1, xlab="% total count per cell",col=scales::hue_pal()(90)[30:1],horizontal=TRUE,ylim=c(0,8))
boxplot( as.matrix(Matrix::t(rel_expression[names(most_expressed[60:31]),])),cex=.1, las=1, xlab="% total count per cell",col=scales::hue_pal()(90)[60:31],horizontal=TRUE,ylim=c(0,8))
boxplot( as.matrix(Matrix::t(rel_expression[names(most_expressed[90:61]),])),cex=.1, las=1, xlab="% total count per cell",col=scales::hue_pal()(90)[90:61],horizontal=TRUE,ylim=c(0,8))
```

You might see that some genes constitute up to 10-30% of the counts from a single cell and the other top genes are mitochondrial and ribosomal genes. It is quite common that nuclear lincRNAs have correlation with quality and mitochondrial reads. Let us assemble some information about such genes, which are important for quality control and downstream filtering.

These genes can serve several purposes in single-cell data analysis, such as computing cell quality metrics (see below), normalize data (see below) and even help account for batch effects (<div style="text-align: right"> [Lin et al (2019) *PNAS*](https://www.pnas.org/content/116/20/9775) </div>).


</p>
</details>

<details>
<summary>**% Mitochondrial genes**</summary>
<p>


Having the data in a suitable format, we can start calculating some quality metrics. We can for example calculate the percentage of mitocondrial and ribosomal genes per cell and add to the metadata. This will be helpfull to visualize them across different metadata parameteres (i.e. datasetID and chemistry version). There are several ways of doing this, and here manually calculate the proportion of mitochondrial reads and add to the metadata table.

Citing from “Simple Single Cell” workflows (Lun, McCarthy & Marioni, 2017): “High proportions are indicative of poor-quality cells (Islam et al. 2014; Ilicic et al. 2016), possibly because of loss of cytoplasmic RNA from perforated cells. The reasoning is that mitochondria are larger than individual transcript molecules and less likely to escape through tears in the cell membrane.”

(PS: non-linear relationship)

```{r, eval=F,  }
# Calculating % mitochondrial genes
SeuratObject <- PercentageFeatureSet(
  object = SeuratObject,
  features = rownames(SeuratObject),
  pattern = "^MT-",
  assay = "RNA",
  col.name = "percent_mito")
```


</p>
</details>

<details>
<summary>**% Ribossomal genes**</summary>
<p>


In the same manner we will calculate the proportion gene expression that comes from ribosomal proteins. Ribossomal genes are the also among the top expressed genes in any cell and, on the contrary to mitochondrial genes, are inversely proportional to the mitochondrial content: the higher the mitochondrial content, the lower is the detection of ribossomal genes (PS: non-linear relationship).

```{r, eval=F,  }
# Calculating % ribossomal genes
SeuratObject <- PercentageFeatureSet(
  SeuratObject, 
  pattern = "^RP[SL]", 
  col.name = "percent_ribo")
```

</p>
</details>

<details>
<summary>**% Gene biotype and chromossome location**</summary>
<p>

In RNA-sequencing, genes can be categorized into different groups depending on their RNA biotype. For example, "coding", "non-coding", "VDJ region genes" are "small interefering RNA" common gene biotypes. Besides, having information about chromossomal location might be usefull to identify bacth effects driven by sex chromossomes.

Depending on the desired type of analysis, some gene categories can be filtered out if not of interest. For single cell specifically, cell libraries are usually constructed using poly-A enrichment and therefore enriching for "protein-coding proteins", which usually contitutes around 80-90% of all available genes.

How to run it:

```{r, eval=F  }
library(biomaRt)

# Retrive mouse gene annotation from ENSEMBL
mart = biomaRt::useMart(
  biomart = "ensembl", 
  dataset = "mmusculus_gene_ensembl",
  host = "apr2020.archive.ensembl.org")

# Retrive the selected attributes mouse gene annotation
annot <- biomaRt::getBM(
  attributes = c(
    "external_gene_name",
    "gene_biotype",
    "chromosome_name"),
  mart = mart)
```

Make sure you are using the right species for your dataset. A list of all species available on can be found using `biomaRt::listDatasets(mart)[,"dataset"]`. All species names are formated in the same way, such as `mmusculus_gene_ensembl` and `hsapiens_gene_ensembl`. For reproducibility reasons, it is also advised to specifically choose a biomart release version, since some genes will be renamed, inserted or deleted from the database. You can do so by running `biomaRt::listEnsemblArchives()`.

```{r, eval=F  }
# Match the gene names with theit respective gene biotype
item <- annot[match(rownames(SeuratObject@assays$RNA@counts) , annot[,1]),"gene_biotype"]
item[is.na(item)] <- "unknown"

# Calculate the percentage of each gene biotype
perc <- rowsum(as.matrix(SeuratObject@assays$RNA@counts) , group=item)
perc <- (t(temp)/Matrix::colSums(SeuratObject@assays$RNA@counts))
o <- order(apply(perc,2,median),decreasing = F)
perc <- perc[,o]

# PLOT percentage of each gene biotype
boxplot( perc*100,outline=F,las=2,main="% reads per cell",col=scales::hue_pal()(100),horizontal=T)


# Add table to the object
gene_biotype_table <- setNames(as.data.frame((perc*100)[,names(sort(table(item),decreasing = T))]),paste0("percent_",names(sort(table(item),decreasing = T))))
SeuratObject@meta.data <- SeuratObject@meta.data[,!(colnames(SeuratObject@meta.data) %in% colnames(gene_biotype_table))]

SeuratObject@meta.data <- cbind(
  SeuratObject@meta.data,
  gene_biotype_table)
```

The code above can also be done again by replacing the string `"gene_biotype"` by `"chromosome_name"`:

```{r, eval=F  }
# Match the gene names with theit respective chromossome location
item <- annot[match(rownames(SeuratObject@assays$RNA@counts) , annot[,1]),"chromosome_name"]
item[is.na(item)] <- "unknown"
item[! item %in% as.character(c(1:23,"X","Y","MT")) ] <- "other"
```

If you want to focus the analysis on only protein-coding genes, for example, you can do it so:

```{r, eval=F,  }
dim(SeuratObject)
sel <- annot[match(rownames(SeuratObject) ,
                   annot[,1]),2] == "protein_coding"
genes_use <- rownames(SeuratObject)[sel]
genes_use <- as.character(na.omit(genes_use))
SeuratObject <- SeuratObject[genes_use,]
dim(SeuratObject)
```

</p>
</details>

<details>
<summary>**Cell cycle scoring**</summary>
<p>


We here perform cell cycle scoring. To score a gene list, the algorithm calculates the difference of mean expression of the given list and the mean expression of reference genes. To build the reference, the function randomly chooses a bunch of genes matching the distribution of the expression of the given list. Cell cycle scoring adds three slots in data, a score for S phase, a score for G2M phase and the predicted cell cycle phase.

How to run it:

```{r, eval=F,  }
SeuratObject <- CellCycleScoring(
  object = SeuratObject,
  g2m.features = cc.genes$g2m.genes,
  s.features = cc.genes$s.genes)

SeuratObject$G1.Score <- 1 - SeuratObject$S.Score - SeuratObject$G2M.Score
```

</p>
</details>

<details>
<summary>**Visualizing all metadata in a PCA plot**</summary>
<p>


Having many metadata parameters to analyse individually makes it a bit hard to visualize the real differences between datasets, batches and experimental conditions. One way to try to combine all this information into one plot is by running dimensionality reduction via principal component analysis (PCA) on the continous variables in the metadata. Thus visualizing on the top principal components (1st and 2nd) reflects how different the datasets are.

```{r, eval=F,  results='hold'}
# Calculate PCA using selected metadata parameters
metadata_use <- grep("perc",colnames(SeuratObject@meta.data),value = T)
metadata_use <- c("nCount_RNA","nFeature_RNA","S.Score","G2M.Score",metadata_use)
PC <- prcomp( SeuratObject@meta.data[,metadata_use] ,center = T, scale. = T)

# Add the PCA (ran on the METADATA) in the object
SeuratObject@reductions[["pca_metadata"]] <- CreateDimReducObject(
  embeddings = PC$x,
  key = "metadataPC_",
  assay = "RNA")

# Plot the PCA ran on the METADATA
DimPlot(SeuratObject,
        reduction = "pca_metadata",
        dims = c(1,2),
        group.by = "orig.ident" )
```

</p>
</details>

<br/>

# Normalization and Regression
***

<details>
<summary>**Normalization**</summary>
<p>

The most common normalization for RNA-seq and also single-cell RNA-seq is log-normalization. This is done by dividing the gene counts of each gene by the sum of all gene counts (a.k.a., library size) to compensate for library size differences. Then the result is multiplied by a constant number, so all cell have the same sequencing depth. For bulk RNA-seq, the constant is usually $1e6$, resulting in CPM (counts per million), but since single-cells library sizes are way lower than that, the number ranges from $1e3$ to $1e4$ (counts per 10000).

$$NormCounts = \frac{GeneCounts * 10000}{LibrarySize}$$
The library size-corrected values are then log-transformed to achieve a log-normal data distribution.

$$logNormCounts = ln(NormCounts+1)$$

How to run it:

```{r, eval=F,  }
SeuratObject <- NormalizeData(
  object = SeuratObject,
  scale.factor = 10000,
  normalization.method = "LogNormalize")
```

</p>
</details>

<details>
<summary>**Scaling and Centering (linear)**</summary>
<p>

Since each gene has a different expression level, it means that genes with higher expression values will naturally have higher variation that will be captured by downstream methods. This means that we need to somehow give each gene a similar weight beforehand (see below). A common practice is to center and scale each gene before performing PCA. This exact scaling is called Z-score normalization it is very useful for PCA, clustering and plotting heatmaps.

Additionally, we can use regression to remove any unwanted sources of variation from the dataset, such as cell cycle, sequencing depth, percent mitocondria. This is achieved by doing a generalized linear regression (GLM) using these parameters as covariates in the model. Then the residuals of the model are taken as the “regressed data”. Although perhaps not in the best way, batch effect regression can also be done here.

How to run it:

```{r, eval=F,  }
SeuratObject <- ScaleData(
  object = SeuratObject,
  vars.to.regress = c("nCount_RNA","mito.percent","nFeatures"),
  model.use = "linear",
  do.scale = T,
  do.center = T)
```


</p>
</details>

<details>
<summary>**Scaling and Centering (poisson)**</summary>
<p>


Since the procedure above assumes a log-linear data distribution, it may be the case that it does not regress the variation correctly, as RNA-seq data (including single cell) relates more closely to a negative bionomial distribution. An alternative variation of the procedure above can also be run on the raw UMI count data but using a "poisson" or "negative binomial" distribution instead. This is performing a gene-wise GLM regression using a poisson model.

How to run it:

```{r, eval=F,  }
SeuratObject <- ScaleData(
  object = SeuratObject,
  vars.to.regress = c("nCount_RNA","mito.percent","nFeatures"),
  model.use = "poisson",
  do.scale = T,
  do.center = T)
```


</p>
</details>

<details>
<summary>**SCtransform**</summary>
<p>


Scaling and centering assuming a poisson distribution might in some cases overfit the data, see above. One can overcome this by pooling information across genes with similar abundances in order to obtain more stable parameter estimates to be used as gene weights in the regression model. This is called "scTransform" and, in simple terms, is performing a gene-wise GLM regression using a constrained negative binomial model.

How to run it:

```{r, eval=F,  }
SeuratObject <- SCTransform( 
  object = SeuratObject,
  assay="RNA",
  vars.to.regress =  c("nCount_RNA","mito.percent","nFeatures"),
  new.assay.name = "sctransform",
  do.center=T )
```


</p>
</details>

<details>
<summary>**Feature selection**</summary>
<p>

An important step in many big-data analysis tasks is to identify features (genes, transcripts, proteins, metabolites, etc) that are actually very variable between the samples being looked at.

For example. Imagine that you have a dataset known to contain different types of epithelial cells, and you use either 1) only genes that are expressed and shared across all epithelial cells at about the same level, 2) only genes that are not detected in epithelial cells, 3) only genes which expression differ greatly across epithelial cells or 4) using all genes. Which of these 4 gene lists can best distinguish the epithelial subtypes in this dataset?

As you could now imagine, using only genes which expression differ greatly across epithelial cells is the best case scenario, followed by using al genes. Therefore, using only genes that are expressed and shared across all epithelial cells at about the same level or only genes that are not detected in epithelial cells do not contain sufficient information to distinguish the epithelial subtypes.

However, since in single-cell we usually do not know the epithelial suptypes the cells before hand (since this is what we want to discover), we need another method to acomplish this task. In general terms, a common approach is to order genes by their overal variance across samples. This is because genes with higher variance will also likely be the ones that can separate the cells the best.

Since genes with higher expression level usually also have naturally higher variation, the gene variation is then normalized by the log  mean expression of each gene (see plot). 

How to run it:

```{r, eval=F,  }
SeuratObject <- FindVariableFeatures(
  object = SeuratObject,
  nfeatures = 3000,
  selection.method = "vst",
  verbose = FALSE,
  assay = "RNA",
  dispersion.function = FastLogVMR,
  mean.function = FastExpMean)
```

Variable gene plot:

```{r, eval=F,  }
top20 <- head(VariableFeatures(alldata), 20)
LabelPoints(plot = VariableFeaturePlot(alldata), points = top20, repel = TRUE)
```

</p>
</details>

<br/>

# Intro to Graphs
***

<details>
<summary>**KNN**</summary>
<p>

KNN refers to “K Nearest Neighbors”, which is a basic and popular topic in data mining and machine learning areas. The KNN graph is a graph in which two vertices p and q are connected by an edge, if the distance between p and q is among the K-th smallest distances.[2] Given different similarity measure of these vectors, the pairwise distance can be Hamming distance, Cosine distance, Euclidean distance and so on. We take Euclidean distance as the way to measure similarity between vectors in this paper. The KNN Graph data structure has many advantages in data mining. For example, for a billion-level dataset, prebuilding a KNN graph offline as an index is much better than doing KNN search online many times.

<div style="text-align: right"> Adapted from [Github](https://github.com/lengyyy/KNN-Graph) </div>

```{r, eval=F,  }
SeuratObject <- FindNeighbors(SeuratObject,
                              assay = "RNA",
                              compute.SNN = F,
                              reduction = "pca",
                              dims = 1:50,
                              graph.name="SNN",
                              prune.SNN = 1/15,
                              k.param = 20,
                              force.recalc = T)
```

Setting `compute.SNN` to `FALSE` will only compute the k-NN graph.

We can take a look at the kNN graph. It is a matrix where every connection between cells is represented as 1s. This is called a unweighted graph (default in Seurat). Some cell connections can however have more importance than others, in that case the scale of the graph from 0
 to a maximum distance. Usually, the smaller the distance, the closer two points are, and stronger is their connection. This is called a weighted graph. Both weighted and unweighted graphs are suitable for clustering, but clustering on unweighted graphs is faster for large datasets (> 100k cells).
 
```{r, eval=F,  }
library(pheatmap)
pheatmap(alldata@graphs$CCA_nn[1:200,1:200],
         col=c("white","black"),border_color = "grey90",
         legend = F,cluster_rows = F,cluster_cols = F,fontsize = 2) 
```


</p>
</details>

<details>
<summary>**SNN**</summary>
<p>

In addition to the k-NN graph, if we then determine the number of nearest neighbors shared by any two points. In graph terminology, we form what we call the "shared nearest neighbor" graph. We do this by replacing the weight of each link between two points (in the nearest neighbor graph) by the number of neighbors that the points share. In other words, this is the number of length 2 paths between any two points in the nearest neighbor graph.

After, this shared nearest neighbor graph is created, all pairs of points are compared and if any two points share more than T neighbors, i.e., have a link in the shared nearest neighbor graph with a weight more than our threshold value, T( TS:. n), then the two points and any cluster they are part of are merged. In other words, clusters are connected components in our shared nearest neighbor graph after we sparsify using a threshold.

How to run it:

```{r, eval=F,  }
SeuratObject <- FindNeighbors(SeuratObject,
                              assay = "RNA",
                              compute.SNN = T,
                              reduction = "pca" ,
                              dims = 1:50,
                              graph.name="SNN",
                              prune.SNN = 1/15,
                              k.param = 20,
                              force.recalc = T)
```

Setting `compute.SNN` to `TRUE` will compute both the k-NN and SNN graphs.


</p>
</details>

<br/>

# Dimensionality reduction
***

<details>
<summary>**PCA**</summary>
<p>

Principal Component Analysis (PCA) is defined as an orthogonal **linear** transformation that transforms the data to a new coordinate system such that **the greatest variance by some scalar projection of the data comes to lie on the first coordinate** (called the first principal component), the second greatest variance on the second coordinate, and so on. […] Often, its operation can be thought of as revealing the internal structure of the data in a way that best explains the variance in the data. […] This is done by **using only the first few principal components** so that the dimensionality of the transformed data is reduced.

<div style="text-align: right"> Adapted from [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis) </div>

How to run it:
```{r, eval=F,  }
SeuratObject <- RunPCA(object = SeuratObject,
                       assay = "RNA",
                       npcs = 100,
                       verbose = FALSE )
```

</p>
</details>

<details>
<summary>**tSNE**</summary>
<p>

<div style="text-align: right"> [Maaten, Hilton (2008) J of Machine Learning Research](http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) </div>


t-distributed stochastic neighborhood embedding (tSNE) is a **nonlinear** dimensionality reduction technique well-suited for embedding high-dimensional data for **visualization** in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that **similar objects are modeled by nearby points** and dissimilar objects are modeled by distant points with high probability. […] t-SNE has been used for visualization in a wide range of applications, including […] bioinformatics […]. While t-SNE plots often seem to display clusters, the **visual clusters can be influenced strongly by the chosen parameterization** and therefore a good understanding of the parameters for t-SNE is necessary. 

<div style="text-align: right"> Adapted from [Wikipedia](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) </div>


Usefull links:

* [How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/)

How to run it:
```{r, eval=F,  }
SeuratObject <- RunTSNE(object = SeuratObject,
                        reduction = "pca",
                        perplexity=30,
                        max_iter=1000,
                        theta=0.5,
                        eta=200,
                        exaggeration_factor=12,
                        dims.use = 1:50,
                        verbose = T,
                        num_threads=0)
```


</p>
</details>

<details>
<summary>**UMAP**</summary>
<p>

Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualisation similarly to t-SNE, but also for general **non-linear** dimension reduction […]. 

<div style="text-align: right"> [umap-learn documentation](https://umap-learn.readthedocs.io/en/latest/) </div>

The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for **visualization** quality, and arguably preserves **more of the global structure** with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning. 

<div style="text-align: right"> [UMAP Arxiv paper](https://arxiv.org/pdf/1802.03426.pdf) </div>

How to run it:
```{r, eval=F,  }
SeuratObject <- RunUMAP(object = SeuratObject,
                        reduction = "pca",
                        dims = 1:top_PCs,
                        n.components = 2,
                        n.neighbors = 20,
                        spread = .3,
                        repulsion.strength = 1,
                        min.dist= .001,
                        verbose = T,
                        num_threads=0,
                        n.epochs = 200,
                        metric = "euclidean",
                        seed.use = 42,
                        reduction.name="umap")
```


</p>
</details>

<details>
<summary>**DM**</summary>
<p>

Diffusion maps (DM) is a dimensionality reduction [...] which computes a family of embeddings of a data set into Euclidean space (often low-dimensional) whose coordinates can be computed from the eigenvectors and eigenvalues of a diffusion operator on the data. The Euclidean distance between points in **the embedded space is equal to the "diffusion distance" between probability distributions** centered at those points. Different from linear dimensionality reduction methods such as principal component analysis (PCA) and multi-dimensional scaling (MDS), diffusion maps is part of the family of **nonlinear** dimensionality reduction methods which focus on discovering the underlying manifold that the data has been sampled from. [...] The basic observation is that **if we take a random walk on the data, walking to a nearby data-point is more likely than walking to another that is far away**.

<div style="text-align: right"> [Wikipedia](https://en.wikipedia.org/wiki/Diffusion_map) </div>


[Diffusion Maps paper](https://www.pnas.org/content/pnas/102/21/7426.full.pdf)

How to run it:
```{r, eval=F,  }
# Load additional libraries
library(destiny)

#Run diffusion maps using the destiny package 
dm <- DiffusionMap( data = SeuratObject@reductions[["pca"]]@cell.embeddings[ , 1:50],
                    k = 20,
                    n_eigs = 20)

#Fix the cell names in the DM embedding
rownames(dm@eigenvectors) <- colnames(SeuratObject)

#Add the DM embbedding to the SeuratObject
SeuratObject@reductions[["dm"]] <- CreateDimReducObject(embeddings = dm@eigenvectors,
                                                        key = "DC_",
                                                        assay = "RNA")
```


</p>
</details>

<details>
<summary>**ICA**</summary>
<p>

Independent Component Analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that the subcomponents are non-Gaussian signals and that they are statistically independent from each other. ICA is a special case of blind source separation.

<div style="text-align: right"> [Wikipedia](https://en.wikipedia.org/wiki/Independent_component_analysis) </div>

How to run it:
```{r, eval=F,  }
SeuratObject <- RunICA(object = SeuratObject,
                       assay = "pca",
                       nics = 20,
                       reduction.name = "ica")

```


</p>
</details>

<br/> 

# Dataset integration
***

Existing batch correction methods were specifically designed for bulk RNA-seq. Thus, their applications to scRNA-seq data assume that the composition of the cell population within each batch is identical. Any systematic differences in the mean gene expression between batches are attributed to technical differences that can be regressed out. However, in practice, population composition is usually not identical across batches in scRNA-seq studies. Even assuming that the same cell types are present in each batch, the abundance of each cell type in the data set can change depending upon subtle differences in cell culture or tissue extraction, dissociation and sorting, etc. Consequently, the estimated coefficients for the batch blocking factors are not purely technical, but contain a non-zero biological component due to differences in composition. Batch correction based on these coefficients will thus yield inaccurate representations of the cellular expression proles, potentially yielding worse results than if no correction was performed.

<div style="text-align: right"> [Haghverdi et al (2018) *Nat Biotechnology*](https://www.nature.com/articles/nbt.4091) </div>

<details>
<summary>**MNN**</summary>
<p>

An alternative approach for data merging and comparison in the presence of batch effects uses a set of landmarks from a reference data set to project new data onto the reference. The rationale here is that a given cell type in the reference batch is most similar to cells of its own type in the new batch. This strategy depends on the selection of landmark points in high dimensional space picked from the reference data set, which cover all cell types that might appear in the later batches. However, if the new batches include cell types that fall outside the transcriptional space explored in the reference batch, these cell types will not be projected to an appropriate position in the space defined by the landmarks. [...] The difference in expression values between cells in a MNN pair provides an estimate of the batch effect, which is made more precise by averaging across many such pairs. A correction vector is obtained from the estimated batch effect and applied to the expression values to perform batch correction. Our approach automatically identifies overlaps in population composition between batches and uses only the overlapping subsets for correction, thus avoiding the assumption of equal composition required by other methods.

The use of MNN pairs involves three assumptions: (i) there is at least one cell population that is present in both batches, (ii) the batch effect is almost orthogonal to the biological subspace, and (iii) batch effect variation is much smaller than the biological effect variation between different cell types.

<div style="text-align: right"> [Haghverdi et al (2018) *Nat Biotechnology*](https://www.nature.com/articles/nbt.4091) </div>

```{r, eval=F,  }
# Load additional libraries
library(SeuratWrappers)

SeuratObject.list <- SplitObject(SeuratObject, split.by = "BATCH")
SeuratObject <- RunFastMNN(object.list = SeuratObject.list,
                           assay = "RNA",
                           features = 2000,
                           reduction.name = "mnn")

# Free memory from working environment
rm( c( SeuratObject.list ) )
gc(verbose = FALSE)
```


</p>
</details>

<details>
<summary>**CCA Anchors**</summary>
<p>

Since MNNs have previously been identified using L2-normalized gene expression, significant differences across batches can obscure the accurate identification of MNNs, particularly when the batch effect is on a similar scale to the biological differences between cell states. To overcome this, we first jointly reduce the dimensionality of both datasets using diagonalized CCA, then apply L2-normalization to the canonical correlation vectors. We next search for MNNs in this shared low-dimensional represen- tation. We refer to the resulting cell pairs as anchors, as they encode the cellular relationships across datasets that will form the basis for all subsequent integration analyses.

Obtaining an accurate set of anchors is paramount to suc- cessful integration. Aberrant anchors that form between different biological cell states across datasets are analogous to noisy edges that occur in k-nearest neighbor (KNN) graphs (Bendall et al., 2014) and can confound downstream analyses. This has motivated the use of shared nearest neighbor (SNN) graphs (Levine et al., 2015; Shekhar et al., 2016), where the similarity between two cells is assessed by the overlap in their local neigh- borhoods. As this measure effectively pools neighbor informa- tion across many cells, the result is robust to aberrant connec- tions in the neighbor graph. We introduced an analogous procedure for the scoring of anchors, where each anchor pair was assigned a score based on the shared overlap of mutual neighborhoods for the two cells in a pair. High-scoring correspondences therefore represent cases where many similar cells in one dataset are predicted to correspond to the same group of similar cells in a second data- set, reflecting increased robustness in the association between the anchor cells. While we initially identify anchors in low-dimen- sional space, we also filter out anchors whose correspondence is not supported based on the original untransformed data.

<div style="text-align: right"> [Stuart et al (2019) *Cell*](https://www.cell.com/cell/fulltext/S0092-8674(19)30559-8) </div>

```{r, eval=F,  }
SeuratObject.list <- SplitObject(
  object = SeuratObject,
  split.by = "BATCH")

SeuratObject.anchors <- FindIntegrationAnchors(
  object.list = SeuratObject.list,
  dims = 1:30)

SeuratObject <- IntegrateData(
  anchorset = SeuratObject.anchors,
  dims = 1:30,
  new.assay.name = "cca")
```


</p>
</details>

<details>
<summary>**LIGER**</summary>
<p>

<div style="text-align: right"> [Welch et al (2019) *Cell*](https://www.cell.com/cell/pdf/S0092-8674(19)30504-5.pdf) </div>

```{r, eval=F,  }

```


</p>
</details>

<details>
<summary>**Conos**</summary>
<p>

<div style="text-align: right"> [Barkas et al (2019) *Nat Methods*](https://www.nature.com/articles/s41592-019-0466-z) </div>

```{r, eval=F,  }
# Load additional libraries
library(conos)
library(SeuratWrappers)

# Split the data per batch to be corrected
SeuratObject.list <- SplitObject(SeuratObject, split.by = "Method")
for (i in 1:length(SeuratObject.list)) {
    SeuratObject.list[[i]] <- 
      NormalizeData(SeuratObject.list[[i]]) %>%
      FindVariableFeatures() %>% 
      ScaleData() %>% 
      RunPCA(verbose = FALSE)
}

# Create a Conos object
SeuratObject.con <- Conos$new(SeuratObject.list)

# Build a joint graph across datasets and find shared communities
SeuratObject.con$buildGraph(
  k = 15, 
  k.self = 5,
  space = "PCA",
  ncomps = 30,
  n.odgenes = 2000,
  matching.method = "mNN",
  metric = "angular",
  score.component.variance = TRUE,
  verbose = TRUE)

SeuratObject.con$findCommunities()

# Create a Joint embedding and conver it back to Seurat Object
SeuratObject.con$embedGraph()
SeuratObject <- as.Seurat(SeuratObject.con)

# Free memory from working environment
rm( c( SeuratObject.con, SeuratObject.list ) )
gc(verbose = FALSE)
```


</p>
</details>

<details>
<summary>**Harmony**</summary>
<p>


<div style="text-align: right"> [Korsunsky et al (2019) *Nat Mathods*](https://www.nature.com/articles/s41592-019-0619-0) </div>


```{r, eval=F,  }
# Load additional libraries
library(harmony)
library(SeuratWrappers)

SeuratObject <- RunHarmony(
  SeuratObject,
  group.by.vars = "Method")
```


</p>
</details>

<br/> 

# Clustering
***

<details>
<summary>**Louvain**</summary>
<p>

The Louvain method for community detection is a method to extract communities from large networks created by Blondel et al. from the University of Louvain. The method is a greedy optimization method that appears to run in time $O(n.log^2n)$ in the number of nodes in the network.The value to be optimized is modularity, defined as a value in the range that measures the density of links inside communities compared to links between communities. Optimizing this value theoretically results in **the best possible grouping of the nodes of a given network**, however going through all possible iterations of the nodes into groups is impractical so heuristic algorithms are used.

<div style="text-align: right"> [Wikipedia](https://en.wikipedia.org/wiki/Independent_component_analysis) </div>


[Louvain Paper](https://iopscience.iop.org/article/10.1088/1742-5468/2008/10/P10008/pdf)

How to run it:
```{r, eval=F,  }
SeuratObject <- FindClusters(
  object = SeuratObject,
  resolution = "0.8",
  algorithm = 1) #algorithim 1 = Louvain
```

The number of clusters can be controled using the `resolution` parameter.

</p>
</details>

<details>
<summary>**Leiden**</summary>
<p>

Leiden algorithm is applied iteratively, it converges to a partition in which all subsets of all communities are locally optimally assigned. Furthermore, by relying on a fast local move approach, the Leiden algorithm runs faster than the Louvain algorithm. The Leiden algorithm consists of three phases: (1) local moving of nodes, (2) refinement of the partition and (3) aggregation of the network based on the refined partition, using the non-refined partition to create an initial partition for the aggregate network.

<div style="text-align: right"> [Leiden Paper](https://www.nature.com/articles/s41598-019-41695-z.pdf) </div>

```{r,eval=F }
SeuratObject <- FindClusters(
  object = SeuratObject,
  resolution = "0.8",
  algorithm = 4)  #algorithim 4 = Louvain
```

The number of clusters can be controled using the `resolution` parameter.

</p>
</details>

<details>
<summary>**Hierachical clustering**</summary>
<p>

Hierachical clustering (HC) is a method of cluster analysis which **seeks to build a hierarchy of clusters**. Strategies for hierarchical clustering generally fall into two types: Agglomerative or Divisive. [...] In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are **usually presented in a dendrogram**. [...] The standard algorithm for hierarchical agglomerative clustering (HAC) has a time complexity of $O(n^3)$ and requires $O(n^2)$ memory, which makes it **too slow for even medium data sets**. In order to decide which clusters should be combined (for agglomerative), or where a cluster should be split (for divisive), **a measure of dissimilarity between sets of observations** is required. In most methods of hierarchical clustering, this is achieved by use of an appropriate metric (a measure of distance between pairs of observations), and **a linkage criterion** which specifies the dissimilarity of sets as a function of the pairwise distances of observations in the sets.

<div style="text-align: right"> [Wikipedia](https://en.wikipedia.org/wiki/Hierarchical_clustering) </div>

[HC for networks](https://en.wikipedia.org/wiki/Hierarchical_clustering_of_networks)

The base R stats package already contains a function `dist()` that calculates distances between all pairs of samples. The distance methods available in `dist()` are: *euclidean*, *maximum*, *manhattan*, *canberra*, *binary* or *minkowski*. Additionally, we can also perform hierarchical clustering directly on a graph (KNN or SNN) which already contains information about cell-to-cell distances. However, since the distances in the graph are inverted ($0$s represent far and $1$s represent close connections), we need to subtract from the maximum value on the graph (in the case of adjacency SNN, is $1$), so that $0$s represent *close* and $1$s represent *far* distance.

After having calculated the distances between samples calculated, we can now proceed with the hierarchical clustering per-se. We will use the function `hclust()` for this purpose, in which we can simply run it with the distance objects created above. The methods available are: *ward.D*, *ward.D2*, *single*, *complete*, *average*, *mcquitty*, *median* or *centroid*. When clustering on a graph, use *ward.D2*.

```{r, eval=F  }
# Running HC on a PCA
h <- hclust(
  d = dist( SeuratObject@reductions["pca"]@cell.embeddings [ , 1:30 ],
            method = "euclidean") ,
  method = "ward.D2")

# Running HC on a graph
h <- hclust( 
  d = 1 - as.dist(SeuratObject@graphs$SNN) ,
  method = "ward.D2",)
```

Once the cluster hierarchy is defined, the next step is to define which samples belong to a particular cluster. However, the sample groups are already known in this example, so clustering them does not add much information for us. What we can do instead is subdivide the genes into clusters. As for the PCA (above), the ideal scenario is to use the Z-score normalized gene expression table, because in this way we make sure that we are grouping together expression trends (going up vs. down), rather than expression level (genes with more counts vs less counts). This way, we can simply repeat the steps above using the transpose of the Z-score matrix, compute the correlation distances and cluster using ward.D2 linkage method.

```{r, eval=F  }
plot(h, labels = F)
```

After identifying the dendrogram for the genes (above), we can now literally cut the tree at a fixed threshold (with the `cutree` function) at different levels (a.k.a. resolutions) to define the clusters.

```{r, eval=F  }
# Cutting the tree based on a height
SeuratObject$HC_res <- cutree(
  tree = h,
  k = 18)

# Cutting the tree based on a number of clusters
SeuratObject$HC_res <- cutree(
  tree = h,
  h = 3)

# To check how many cells are in each cluster
table(SeuratObject$HC_res)
```

The number of clusters can be controled using the height (`h`) or directly via the `k` parameters.

</p>
</details>

<details>
<summary>**K-mean**</summary>
<p>

k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition $n$ observations into $k$ clusters in which **each observation belongs to the cluster with the nearest mean** (cluster centers or cluster centroid), serving as a prototype of the cluster. [...] The algorithm does not guarantee convergence to the global optimum. The result may depend on the initial clusters. As the algorithm is usually fast, **it is common to run it multiple times** with different starting conditions. [...] k-means clustering tends to find clusters of **comparable spatial extent (all with same size)**, while the expectation-maximization mechanism allows clusters to have different shapes.
<div style="text-align: right"> [Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering) </div>

K-means is a generic clustering algorithm that has been used in many application areas. In R, it can be applied via the kmeans function. Typically, it is applied to a reduced dimension representation of the expression data (most often PCA, because of the interpretability of the low-dimensional distances). We need to define the number of clusters in advance. Since the results depend on the initialization of the cluster centers, it is typically recommended to run K-means with multiple starting configurations (via the `nstart` argument).

```{r, eval=F,  }
set.seed(1)
SeuratObject$kmeans_12 <- kmeans(
  x = SeuratObject@reductions[["pca"]]@cell.embeddings [ , 1:50 ],
  centers = 12,
  iter.max = 50,
  nstart = 10)$cluster
```

The number of clusters can be controled using the `centers` parameter.


</p>
</details>

<details>
<summary>**Affinity propagation**</summary>
<p>


In statistics and data mining, affinity propagation (AP) is a clustering algorithm based on the concept of "message passing" between data points. Unlike clustering algorithms such as k-means or k-medoids, affinity propagation **does not require the number of clusters to be determined** or estimated before running the algorithm. Similar to k-medoids, affinity propagation finds "exemplars," members of the input set that are representative of clusters. [...] Iterations are performed until either the cluster boundaries remain unchanged over a number of iterations, or some predetermined number (of interations) is reached.

<div style="text-align: right"> [Wikipedia](https://en.wikipedia.org/wiki/Affinity_propagation) </div>


</p>
</details>

<br/> 

# Differential expression
***

<details>
<summary>**Finding Cluster Markers**</summary>
<p>

</p>
</details>

<details>
<summary>**Comparing a cluster across experimental conditions**</summary>
<p>

</p>
</details>

<details>
<summary>**Plotting DEG results**</summary>
<p>

</p>
</details>

<br/>

# Trajectory inference
***

<details>
<summary>**Slingshot**</summary>
<p>

</p>
</details>

<details>
<summary>**Differential expression along trajectories**</summary>
<p>

</p>
</details>


<br/>

<br/>

### [Back to main](README.md)